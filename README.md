# Misconception-Answer-Affinity-Prediction-in-MCQs

This project aims to develop an NLP-based machine learning model that predicts the affinity between misconceptions and distractors in multiple-choice diagnostic questions. A diagnostic question presents four options, with one correct answer and three distractors, each designed to capture a specific misconception. Tagging these distractors with appropriate misconceptions is essential for educational diagnostics but poses challenges in terms of time, consistency, and scalability. This project seeks to create an automated, efficient solution to suggest potential misconceptions for each distractor, reducing the burden on human annotators while improving tagging accuracy and consistency. Unlike previous attempts that struggled with pre-trained models on complex mathematical language, our approach aims to leverage an ML model that captures both established and emerging misconceptions across various topic areas. By facilitating consistent and precise tagging of distractors, this model will enhance diagnostic question quality, benefiting educators and students alike. We have used baselines like RandomforestClassier and used BERT base. We also hope to use LLM models like Gemma, Llama, Mistral, GPT4 etc to do the task. The competition is a unique opportunity to address a significant need in educational assessment, with the potential to improve understanding and management of student misconceptions in diverse subject areas.
