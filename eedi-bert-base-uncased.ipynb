{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":129073,"sourceType":"modelInstanceVersion","modelInstanceId":108753,"modelId":133071}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:12.298698Z","iopub.execute_input":"2024-12-09T10:56:12.299505Z","iopub.status.idle":"2024-12-09T10:56:12.629738Z","shell.execute_reply.started":"2024-12-09T10:56:12.299467Z","shell.execute_reply":"2024-12-09T10:56:12.628947Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/eedi-mining-misconceptions-in-mathematics/sample_submission.csv\n/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\n/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\n/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/config.json\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/README.md\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/tf_model.h5\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/tokenizer_config.json\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/gitattributes\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/pytorch_model.bin\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/model.safetensors\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/special_tokens_map.json\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/vocab.txt\n/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased/flax_model.msgpack\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW, get_scheduler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport os\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:12.631053Z","iopub.execute_input":"2024-12-09T10:56:12.631400Z","iopub.status.idle":"2024-12-09T10:56:31.246260Z","shell.execute_reply.started":"2024-12-09T10:56:12.631374Z","shell.execute_reply":"2024-12-09T10:56:31.245175Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import random\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.247500Z","iopub.execute_input":"2024-12-09T10:56:31.248213Z","iopub.status.idle":"2024-12-09T10:56:31.350383Z","shell.execute_reply.started":"2024-12-09T10:56:31.248169Z","shell.execute_reply":"2024-12-09T10:56:31.349463Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df_actual = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv\")\nreal_test_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv\")\nmisconception_df = pd.read_csv(\"/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.352858Z","iopub.execute_input":"2024-12-09T10:56:31.353242Z","iopub.status.idle":"2024-12-09T10:56:31.409309Z","shell.execute_reply.started":"2024-12-09T10:56:31.353203Z","shell.execute_reply":"2024-12-09T10:56:31.408618Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split into train, test, and validation\ntrain_df, test_df = train_test_split(train_df_actual, test_size=0.15, random_state=42)  # 15% for testing\ntrain_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)  # 15% of remaining for validation\n\nprint(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.410340Z","iopub.execute_input":"2024-12-09T10:56:31.410610Z","iopub.status.idle":"2024-12-09T10:56:31.428476Z","shell.execute_reply.started":"2024-12-09T10:56:31.410583Z","shell.execute_reply":"2024-12-09T10:56:31.427510Z"}},"outputs":[{"name":"stdout","text":"Train size: 1349, Validation size: 239, Test size: 281\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import re\n\n# Function to preprocess LaTeX into plain text\ndef preprocess_latex_to_text(latex_str):\n    # Handle common LaTeX functions and symbols\n    latex_str = latex_str.replace(r'\\frac', 'over')  # Convert fractions\n    latex_str = latex_str.replace(r'\\sum', 'sum')  # Convert summation symbol\n    latex_str = latex_str.replace(r'\\int', 'integral')  # Convert integral symbol\n    latex_str = latex_str.replace(r'\\sqrt', 'square root')  # Convert square root\n    latex_str = latex_str.replace(r'\\text', '')  # Remove text formatting in LaTeX\n    \n    # Handle superscripts and subscripts (e.g., x^2 or x_1)\n    latex_str = re.sub(r'\\^{(.*?)}', r' raised to \\1', latex_str)  # e.g., x^{2} becomes x raised to 2\n    latex_str = re.sub(r'_{(.*?)}', r' sub \\1', latex_str)  # e.g., x_{1} becomes x sub 1\n    \n    # Remove other LaTeX math environments (e.g., dollar signs for inline math)\n    latex_str = latex_str.replace('$', '')\n    \n    # Optionally, remove other LaTeX-specific symbols or escape characters\n    latex_str = latex_str.replace(r'\\\\', '')  # Remove LaTeX newlines\n    latex_str = latex_str.replace(r'{', ' ').replace(r'}', ' ')  # Remove curly braces\n    latex_str = ' '.join(latex_str.split())  # Clean up extra spaces\n    \n    return latex_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.429613Z","iopub.execute_input":"2024-12-09T10:56:31.430224Z","iopub.status.idle":"2024-12-09T10:56:31.439226Z","shell.execute_reply.started":"2024-12-09T10:56:31.430186Z","shell.execute_reply":"2024-12-09T10:56:31.438455Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Combine question and answer text\ndef preprocess_data(df):\n    data = []\n    for _, row in df.iterrows():\n        for option in [\"A\", \"B\", \"C\", \"D\"]:  # Only incorrect answers\n            # if row['CorrectAnswer'] != option:\n            input_text = f\"Question: {row['QuestionText']} | Answer: {row[f'Answer{option}Text']}\"\n            input_text = preprocess_latex_to_text(input_text)\n            label = row[f\"Misconception{option}Id\"]\n            questionid = f\"{row['QuestionId']}\"\n            answer = f\"{option}\"\n            data.append((questionid, answer, input_text, label))\n    return pd.DataFrame(data, columns=[\"QuestionId\",\"Answer\", \"text\", \"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.440041Z","iopub.execute_input":"2024-12-09T10:56:31.440302Z","iopub.status.idle":"2024-12-09T10:56:31.450000Z","shell.execute_reply.started":"2024-12-09T10:56:31.440278Z","shell.execute_reply":"2024-12-09T10:56:31.449315Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# class MisconceptionDataset(Dataset):\n#     def __init__(self, texts, labels, tokenizer, max_length=128):\n#         self.texts = texts\n#         self.labels = labels\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         text = self.texts[idx]\n#         label = self.labels[idx]\n\n#         # Assuming `text` has two parts: question and answer\n#         question, answer = text.split(\" | Answer: \")\n\n#         # Tokenize with [CLS] and [SEP] tokens\n#         tokens = self.tokenizer(\n#             question,\n#             answer,\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=self.max_length,\n#             return_tensors=\"pt\",\n#             return_special_tokens_mask=True,  # Helps ensure correct use of [SEP]\n#         )\n#         return {\n#             \"input_ids\": tokens[\"input_ids\"].squeeze(),\n#             \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n#             \"labels\": torch.tensor(label, dtype=torch.float),\n#         }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.450836Z","iopub.execute_input":"2024-12-09T10:56:31.451068Z","iopub.status.idle":"2024-12-09T10:56:31.465271Z","shell.execute_reply.started":"2024-12-09T10:56:31.451045Z","shell.execute_reply":"2024-12-09T10:56:31.464537Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class MisconceptionDataset(Dataset):\n    def __init__(self, question_ids, answer_labels, texts, labels, tokenizer, max_length=512):\n        \"\"\"\n        Args:\n            question_ids (list): List of Question IDs.\n            answer_labels (list): List of Answer Labels (e.g., A, B, C, D).\n            texts (list): List of question texts.\n            labels (list): List of true Misconception IDs.\n            tokenizer (transformers tokenizer): Tokenizer to encode the text.\n            max_length (int, optional): Max sequence length for tokenization. Defaults to 512.\n        \"\"\"\n        self.question_ids = question_ids\n        self.answer_labels = answer_labels\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        question_id = self.question_ids[idx]\n        answer_label = self.answer_labels[idx]\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # Assuming `text` has two parts: question and answer\n        question, answer = text.split(\" | Answer: \")\n        question = question.split(\"Question: \")[1]\n\n        # Tokenize with [CLS] and [SEP] tokens\n        tokens = self.tokenizer(\n            question,\n            answer,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n            return_special_tokens_mask=True,  # Helps ensure correct use of [SEP]\n        )\n        \n        # # Assuming the `text` is just the question text; answer is inferred from `answer_label`\n        # question = text  # In case answer text is separate, modify as needed\n\n        # # Tokenize the question text\n        # tokens = self.tokenizer(\n        #     question,\n        #     padding=\"max_length\",\n        #     truncation=True,\n        #     max_length=self.max_length,\n        #     return_tensors=\"pt\",\n        #     return_special_tokens_mask=True,  # Ensures correct use of [SEP]\n        # )\n\n        return {\n            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n            \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label, dtype=torch.float),\n            \"QuestionId\": question_id,  # Include the Question ID\n            \"AnswerLabel\": answer_label  # Include the Answer label (A, B, C, D)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.466269Z","iopub.execute_input":"2024-12-09T10:56:31.466588Z","iopub.status.idle":"2024-12-09T10:56:31.476332Z","shell.execute_reply.started":"2024-12-09T10:56:31.466551Z","shell.execute_reply":"2024-12-09T10:56:31.475668Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Preprocess data\ntrain_data = preprocess_data(train_df)\nval_data = preprocess_data(val_df)\ntest_data = preprocess_data(test_df)\n\ntrain_data.dropna(inplace = True)\nval_data.dropna(inplace = True)\ntest_data.dropna(inplace = True)\n\n# # Combine into DataFrames\n# train_data = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n# val_data = pd.DataFrame(val_data, columns=[\"text\", \"label\"])\n# test_data = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n\n# Convert labels to multi-hot encoding\nall_labels = sorted(misconception_df[\"MisconceptionId\"].unique())  # Get all unique labels\nmlb = MultiLabelBinarizer(classes=all_labels)\n\ntrain_labels = mlb.fit_transform([[label] for label in train_data[\"label\"]])\nval_labels = mlb.transform([[label] for label in val_data[\"label\"]])\ntest_labels = mlb.transform([[label] for label in test_data[\"label\"]])\n\ntrain_data.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.478925Z","iopub.execute_input":"2024-12-09T10:56:31.479230Z","iopub.status.idle":"2024-12-09T10:56:31.766317Z","shell.execute_reply.started":"2024-12-09T10:56:31.479189Z","shell.execute_reply":"2024-12-09T10:56:31.765388Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   QuestionId Answer                                               text  \\\n0         368      A  Question: Without using a calculator, which tw...   \n7        1043      D  Question: Which signs belong in the boxes to m...   \n8        1340      A  Question: A square has an area of \\( 100 \\math...   \n10       1340      C  Question: A square has an area of \\( 100 \\math...   \n12        586      A  Question: Simplify \\( square root 48 \\) as muc...   \n\n     label  \n0    734.0  \n7   2030.0  \n8   1678.0  \n10   734.0  \n12  2384.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QuestionId</th>\n      <th>Answer</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>368</td>\n      <td>A</td>\n      <td>Question: Without using a calculator, which tw...</td>\n      <td>734.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1043</td>\n      <td>D</td>\n      <td>Question: Which signs belong in the boxes to m...</td>\n      <td>2030.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1340</td>\n      <td>A</td>\n      <td>Question: A square has an area of \\( 100 \\math...</td>\n      <td>1678.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1340</td>\n      <td>C</td>\n      <td>Question: A square has an area of \\( 100 \\math...</td>\n      <td>734.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>586</td>\n      <td>A</td>\n      <td>Question: Simplify \\( square root 48 \\) as muc...</td>\n      <td>2384.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.767178Z","iopub.execute_input":"2024-12-09T10:56:31.767430Z","iopub.status.idle":"2024-12-09T10:56:31.772173Z","shell.execute_reply.started":"2024-12-09T10:56:31.767406Z","shell.execute_reply":"2024-12-09T10:56:31.771184Z"}},"outputs":[{"name":"stdout","text":"Train size: 3160, Validation size: 555, Test size: 655\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Create datasets\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = MisconceptionDataset(train_data[\"QuestionId\"].tolist(), train_data[\"Answer\"].tolist(), train_data[\"text\"].tolist(), train_labels, tokenizer)\nval_dataset = MisconceptionDataset(val_data[\"QuestionId\"].tolist(), val_data[\"Answer\"].tolist(), val_data[\"text\"].tolist(), val_labels, tokenizer)\ntest_dataset = MisconceptionDataset(test_data[\"QuestionId\"].tolist(), test_data[\"Answer\"].tolist(), test_data[\"text\"].tolist(), test_labels, tokenizer)\n\n# DataLoader for batching\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\nval_dataloader = DataLoader(val_dataset, batch_size=32)\ntest_dataloader = DataLoader(test_dataset, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:31.773355Z","iopub.execute_input":"2024-12-09T10:56:31.774213Z","iopub.status.idle":"2024-12-09T10:56:32.477215Z","shell.execute_reply.started":"2024-12-09T10:56:31.774170Z","shell.execute_reply":"2024-12-09T10:56:32.476564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63668d661308491396e2ade6917eaae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213d59200d41444787d72e5787641a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bda35505c995483f9c3deb1cb2363009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f64fb9295ee4eaeb006a1a4f8bdfb32"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# text = train_data['text'][30]\n# question, answer = text.split(\" | Answer: \")\n# tokens = tokenizer(\n#             question,\n#             answer,\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=128,\n#             return_tensors=\"pt\"\n#             # return_special_tokens_mask=True,  # Helps ensure correct use of [SEP]\n#         )\n\n# decoded_tokens = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n# print(\"Tokens:\", decoded_tokens)\n# print(question.split(\"Question: \")[1])\n# print(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:32.478231Z","iopub.execute_input":"2024-12-09T10:56:32.478489Z","iopub.status.idle":"2024-12-09T10:56:32.482513Z","shell.execute_reply.started":"2024-12-09T10:56:32.478463Z","shell.execute_reply":"2024-12-09T10:56:32.481711Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:32.483481Z","iopub.execute_input":"2024-12-09T10:56:32.483719Z","iopub.status.idle":"2024-12-09T10:56:32.493522Z","shell.execute_reply.started":"2024-12-09T10:56:32.483695Z","shell.execute_reply":"2024-12-09T10:56:32.492782Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(all_labels), ignore_mismatched_sizes=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.001)\nnum_training_steps = len(train_dataloader) * 10  # 10 epochs\nscheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:32.494557Z","iopub.execute_input":"2024-12-09T10:56:32.494795Z","iopub.status.idle":"2024-12-09T10:56:36.242424Z","shell.execute_reply.started":"2024-12-09T10:56:32.494744Z","shell.execute_reply":"2024-12-09T10:56:36.241552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e850a989642946608bccd3f65383d403"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from torch.nn import BCEWithLogitsLoss\nfrom torch.nn import CrossEntropyLoss\n\n# Change the loss calculation in training and evaluation loops\nloss_fn = CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:36.243904Z","iopub.execute_input":"2024-12-09T10:56:36.244345Z","iopub.status.idle":"2024-12-09T10:56:36.249010Z","shell.execute_reply.started":"2024-12-09T10:56:36.244301Z","shell.execute_reply":"2024-12-09T10:56:36.248197Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def calculate_precision_at_k(predictions, true_labels, k):\n    \"\"\"\n    Calculate Precision at rank k.\n    predictions: List of predicted MisconceptionId(s) for a particular QuestionId_Answer.\n    true_labels: Set of correct MisconceptionId(s) for the same QuestionId_Answer.\n    k: The rank at which precision is calculated.\n    \"\"\"\n    relevant = sum([1 for pred in predictions[:k] if pred == true_labels])\n    return relevant / k\n\ndef evaluate_map_at_25(predictions, true_labels, n=25):\n    \"\"\"\n    Evaluate Mean Average Precision at 25 (MAP@25).\n    predictions: Dictionary of predictions for each QuestionId_Answer.\n    true_labels: Dictionary of true MisconceptionId(s) for each QuestionId_Answer.\n    n: Number of predictions per observation (default is 25).\n    \"\"\"\n    map_score = 0\n    U = len(predictions)  # Number of observations (test set size)\n\n    for question_answer, pred_list in predictions.items():\n        true_set = true_labels.get(question_answer, [])\n        \n        if len(true_set) == 0:\n            continue  # Skip this if there are no true labels\n        true_set = np.argmax(true_set)\n        # Store the correct labels found\n        relevant_found = set()\n        avg_precision = 0\n        \n        for k in range(1, min(len(pred_list), n) + 1):\n            precision = 0\n            # Check if the prediction at rank k is relevant and not already counted\n            if pred_list[k - 1] == true_set and pred_list[k - 1] not in relevant_found:\n                relevant_found.add(pred_list[k - 1])\n                precision = calculate_precision_at_k(pred_list, true_set, k)\n            \n            # Precision at rank k: how many relevant items are in the top k\n            # precision = len(relevant_found) / k\n            avg_precision += precision\n        \n        map_score += avg_precision\n\n    # Compute Mean Average Precision (MAP@25)\n    return map_score / U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:36.250308Z","iopub.execute_input":"2024-12-09T10:56:36.250885Z","iopub.status.idle":"2024-12-09T10:56:36.261889Z","shell.execute_reply.started":"2024-12-09T10:56:36.250856Z","shell.execute_reply":"2024-12-09T10:56:36.261032Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Training loop with validation\nbest_val_loss = float(\"inf\")\nbest_model_path = \"best_model.pth\"\n\nfor epoch in range(10):  # Number of epochs\n    model.train()\n    train_loss = 0\n    for batch in train_dataloader:\n        predictions, true_labels = [], []\n        # Store MAP@25 results\n        all_predictions = {}\n        all_true_labels = {}\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        logits = outputs.logits\n        probabilities = torch.softmax(logits, dim=-1)\n\n        # Get top 25 predictions (this can be adjusted for the actual number of misconceptions)\n        top_k_predictions = torch.topk(probabilities, k=25, dim=1).indices.cpu().numpy()\n        \n        # Store predictions and true labels\n        for i, question_id in enumerate(batch[\"QuestionId\"]):  # Ensure batch contains 'QuestionId'\n            question_answer = f\"{question_id}_{batch['AnswerLabel'][i]}\"  # Format QuestionId_Answer for unique identifier\n            \n            all_predictions[question_answer] = top_k_predictions[i]  # Store top 25 predicted misconception ids\n            all_true_labels[question_answer] = labels[i].cpu().numpy()  # Store true misconception ids\n\n        map_at_25 = evaluate_map_at_25(all_predictions, all_true_labels)\n        loss = loss_fn(logits, labels) + 0.5*(1- map_at_25)\n        # loss = outputs.loss\n        train_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    train_loss /= len(train_dataloader)\n    print(f\"Epoch {epoch + 1} Training Loss: {train_loss:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            predictions, true_labels = [], []\n            # Store MAP@25 results\n            all_predictions = {}\n            all_true_labels = {}\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            logits = outputs.logits\n            probabilities = torch.softmax(logits, dim=-1)\n            # Get top 25 predictions (this can be adjusted for the actual number of misconceptions)\n            top_k_predictions = torch.topk(probabilities, k=25, dim=1).indices.cpu().numpy()\n            \n            # Store predictions and true labels\n            for i, question_id in enumerate(batch[\"QuestionId\"]):  # Ensure batch contains 'QuestionId'\n                question_answer = f\"{question_id}_{batch['AnswerLabel'][i]}\"  # Format QuestionId_Answer for unique identifier\n                \n                all_predictions[question_answer] = top_k_predictions[i]  # Store top 25 predicted misconception ids\n                all_true_labels[question_answer] = labels[i].cpu().numpy()  # Store true misconception ids\n\n            val_loss += (loss_fn(logits, labels) + 0.5*(1- map_at_25)).item()\n            # val_loss += outputs.loss.item()\n\n    val_loss /= len(val_dataloader)\n    print(f\"Epoch {epoch + 1} Validation Loss: {val_loss:.4f}\")\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Saved best model at epoch {epoch + 1}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:56:36.263178Z","iopub.execute_input":"2024-12-09T10:56:36.263494Z","iopub.status.idle":"2024-12-09T11:50:48.284606Z","shell.execute_reply.started":"2024-12-09T10:56:36.263455Z","shell.execute_reply":"2024-12-09T11:50:48.283715Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Training Loss: 8.2078\nEpoch 1 Validation Loss: 7.9731\nSaved best model at epoch 1.\nEpoch 2 Training Loss: 7.5285\nEpoch 2 Validation Loss: 7.9325\nSaved best model at epoch 2.\nEpoch 3 Training Loss: 7.2385\nEpoch 3 Validation Loss: 7.7617\nSaved best model at epoch 3.\nEpoch 4 Training Loss: 6.9502\nEpoch 4 Validation Loss: 7.6554\nSaved best model at epoch 4.\nEpoch 5 Training Loss: 6.6866\nEpoch 5 Validation Loss: 7.5593\nSaved best model at epoch 5.\nEpoch 6 Training Loss: 6.4483\nEpoch 6 Validation Loss: 7.4235\nSaved best model at epoch 6.\nEpoch 7 Training Loss: 6.2705\nEpoch 7 Validation Loss: 7.3336\nSaved best model at epoch 7.\nEpoch 8 Training Loss: 6.1250\nEpoch 8 Validation Loss: 7.2103\nSaved best model at epoch 8.\nEpoch 9 Training Loss: 6.0202\nEpoch 9 Validation Loss: 7.2880\nEpoch 10 Training Loss: 5.9477\nEpoch 10 Validation Loss: 7.2056\nSaved best model at epoch 10.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# # Evaluate on test set\n\n# model.load_state_dict(torch.load(best_model_path))\n# model.eval()\n\n# test_loss = 0\n# predictions, true_labels = [], []\n\n# with torch.no_grad():\n#     for batch in test_dataloader:\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"labels\"].to(device)\n\n#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n#         test_loss += outputs.loss.item()\n\n#         logits = outputs.logits\n#         probabilities = torch.sigmoid(logits)\n#         predictions.extend((probabilities > 0.5).cpu().numpy())\n#         true_labels.extend(labels.cpu().numpy())\n\n# test_loss /= len(test_dataloader)\n# accuracy = accuracy_score(true_labels, predictions)\n# print(f\"Test Loss: {test_loss:.4f}\")\n# print(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:50:48.285850Z","iopub.execute_input":"2024-12-09T11:50:48.286141Z","iopub.status.idle":"2024-12-09T11:50:48.290399Z","shell.execute_reply.started":"2024-12-09T11:50:48.286099Z","shell.execute_reply":"2024-12-09T11:50:48.289597Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Assuming you have already loaded the model and test_dataloader\nmodel.load_state_dict(torch.load(best_model_path, weights_only=True))\nmodel.eval()\n\ntest_loss = 0\npredictions, true_labels = [], []\n\n# Store MAP@25 results\nall_predictions = {}\nall_true_labels = {}\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        test_loss += outputs.loss.item()\n\n        logits = outputs.logits\n        # print(logits.shape)\n        probabilities = torch.softmax(logits, dim=-1)\n\n        # Get top 25 predictions (this can be adjusted for the actual number of misconceptions)\n        top_k_predictions = torch.topk(probabilities, k=25, dim=1).indices.cpu().numpy()\n        \n        # Store predictions and true labels\n        for i, question_id in enumerate(batch[\"QuestionId\"]):  # Ensure batch contains 'QuestionId'\n            question_answer = f\"{question_id}_{batch['AnswerLabel'][i]}\"  # Format QuestionId_Answer for unique identifier\n            \n            all_predictions[question_answer] = top_k_predictions[i]  # Store top 25 predicted misconception ids\n            all_true_labels[question_answer] = labels[i].cpu().numpy()  # Store true misconception ids\n\n# # Calculate MAP@25\n# def calculate_precision_at_k(predictions, true_labels, k):\n#     \"\"\"\n#     Calculate Precision at rank k.\n#     predictions: List of predicted MisconceptionId(s) for a particular QuestionId_Answer.\n#     true_labels: Set of correct MisconceptionId(s) for the same QuestionId_Answer.\n#     k: The rank at which precision is calculated.\n#     \"\"\"\n#     relevant = sum([1 for pred in predictions[:k] if pred in true_labels])\n#     return relevant / k\n\n# def evaluate_map_at_25(predictions, true_labels, n=25):\n#     \"\"\"\n#     Evaluate Mean Average Precision at 25 (MAP@25).\n#     predictions: Dictionary of predictions for each QuestionId_Answer.\n#     true_labels: Dictionary of true MisconceptionId(s) for each QuestionId_Answer.\n#     n: Number of predictions per observation (default is 25).\n#     \"\"\"\n#     map_score = 0\n#     U = len(predictions)  # Number of observations (test set size)\n\n#     for question_answer, pred_list in predictions.items():\n#         true_set = true_labels.get(question_answer, [])\n        \n#         if len(true_set) == 0:\n#             continue  # Skip this if there are no true labels\n        \n#         avg_precision = 0\n#         for k in range(1, min(len(pred_list), n) + 1):\n#             precision = calculate_precision_at_k(pred_list, true_set, k)\n#             avg_precision += precision\n        \n#         map_score += avg_precision / min(len(pred_list), n)\n\n#     # Compute Mean Average Precision (MAP@25)\n#     return map_score / U\n\ndef calculate_precision_at_k(predictions, true_labels, k):\n    \"\"\"\n    Calculate Precision at rank k.\n    predictions: List of predicted MisconceptionId(s) for a particular QuestionId_Answer.\n    true_labels: Set of correct MisconceptionId(s) for the same QuestionId_Answer.\n    k: The rank at which precision is calculated.\n    \"\"\"\n    relevant = sum([1 for pred in predictions[:k] if pred == true_labels])\n    return relevant / k\n\ndef evaluate_map_at_25(predictions, true_labels, n=25):\n    \"\"\"\n    Evaluate Mean Average Precision at 25 (MAP@25).\n    predictions: Dictionary of predictions for each QuestionId_Answer.\n    true_labels: Dictionary of true MisconceptionId(s) for each QuestionId_Answer.\n    n: Number of predictions per observation (default is 25).\n    \"\"\"\n    map_score = 0\n    U = len(predictions)  # Number of observations (test set size)\n\n    for question_answer, pred_list in predictions.items():\n        true_set = true_labels.get(question_answer, [])\n        \n        if len(true_set) == 0:\n            continue  # Skip this if there are no true labels\n        true_set = np.argmax(true_set)\n        # Store the correct labels found\n        relevant_found = set()\n        avg_precision = 0\n        \n        for k in range(1, min(len(pred_list), n) + 1):\n            precision = 0\n            # Check if the prediction at rank k is relevant and not already counted\n            if pred_list[k - 1] == true_set and pred_list[k - 1] not in relevant_found:\n                relevant_found.add(pred_list[k - 1])\n                precision = calculate_precision_at_k(pred_list, true_set, k)\n            \n            # Precision at rank k: how many relevant items are in the top k\n            # precision = len(relevant_found) / k\n            avg_precision += precision\n        \n        map_score += avg_precision\n\n    # Compute Mean Average Precision (MAP@25)\n    return map_score / U\n\n\n\n# Evaluate MAP@25\nmap_at_25 = evaluate_map_at_25(all_predictions, all_true_labels)\ntest_loss /= len(test_dataloader)\n# print(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"MAP@25: {map_at_25:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:50:48.291502Z","iopub.execute_input":"2024-12-09T11:50:48.291937Z","iopub.status.idle":"2024-12-09T11:51:13.708849Z","shell.execute_reply.started":"2024-12-09T11:50:48.291866Z","shell.execute_reply":"2024-12-09T11:51:13.707918Z"}},"outputs":[{"name":"stdout","text":"MAP@25: 0.1107\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1#MAP@3-Metric\ndef map_at_25(predictions, labels):\n    map_sum = 0\n    # U = len(predictions)  # Number of observations (test set size)\n\n    for question_answer, pred_list in predictions.items():\n        true_set1 = labels.get(question_answer, [])\n        true_set = np.argmax(true_set1)\n        # print(true_set1)\n        # print(pred_list)\n    # for x, y in zip(predictions, labels):\n    #     z = [1 / i if y == j else 0 for i, j in zip(range(1, 26), x)]\n        z = [1 / i if true_set == j else 0 for i, j in zip(range(1, 26), pred_list)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:06:53.661540Z","iopub.execute_input":"2024-12-09T12:06:53.662378Z","iopub.status.idle":"2024-12-09T12:06:53.667357Z","shell.execute_reply.started":"2024-12-09T12:06:53.662341Z","shell.execute_reply":"2024-12-09T12:06:53.666481Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"map_at_25_score = map_at_25(all_predictions, all_true_labels)\nmap_at_25_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:06:56.413840Z","iopub.execute_input":"2024-12-09T12:06:56.414228Z","iopub.status.idle":"2024-12-09T12:06:56.438077Z","shell.execute_reply.started":"2024-12-09T12:06:56.414196Z","shell.execute_reply":"2024-12-09T12:06:56.437013Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.11065142613508325"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# from sklearn.metrics import average_precision_score\n\n# def evaluate_map_at_25(predictions, true_labels, n=25):\n#     \"\"\"\n#     Evaluate Mean Average Precision at 25 (MAP@25).\n#     predictions: Dictionary of predictions for each QuestionId_Answer.\n#     true_labels: Dictionary of true MisconceptionId(s) for each QuestionId_Answer.\n#     n: Number of predictions per observation (default is 25).\n#     \"\"\"\n#     all_avg_precision = []\n\n#     for question_answer, pred_list in predictions.items():\n#         true_set = true_labels.get(question_answer, [])\n        \n#         if len(true_set) == 0:\n#             continue  # Skip this if there are no true labels\n        \n#         # Only consider top-k predictions\n#         relevant_predictions = pred_list[:n]\n        \n#         # Calculate precision using sklearn's average_precision_score\n#         avg_precision = average_precision_score([1 if label in relevant_predictions else 0 for label in true_set], \n#                                                  [1 if pred in relevant_predictions else 0 for pred in relevant_predictions])\n#         all_avg_precision.append(avg_precision)\n\n#     # Compute Mean Average Precision (MAP@25)\n#     return np.mean(all_avg_precision)\n\n# # Evaluate MAP@25\n# map_at_25 = evaluate_map_at_25(all_predictions, all_true_labels)\n# test_loss /= len(test_dataloader)\n# print(f\"Test Loss: {test_loss:.4f}\")\n# print(f\"MAP@25: {map_at_25:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.153919Z","iopub.status.idle":"2024-12-09T11:51:14.154216Z","shell.execute_reply.started":"2024-12-09T11:51:14.154066Z","shell.execute_reply":"2024-12-09T11:51:14.154081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# all_true_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.155542Z","iopub.status.idle":"2024-12-09T11:51:14.156255Z","shell.execute_reply.started":"2024-12-09T11:51:14.155991Z","shell.execute_reply":"2024-12-09T11:51:14.156017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# all_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.157413Z","iopub.status.idle":"2024-12-09T11:51:14.157749Z","shell.execute_reply.started":"2024-12-09T11:51:14.157573Z","shell.execute_reply":"2024-12-09T11:51:14.157590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class PredictionDataset(Dataset):\n#     def __init__(self, question_ids, answer_labels, texts, tokenizer, max_length=128):\n#         \"\"\"\n#         Args:\n#             question_ids (list): List of Question IDs.\n#             answer_labels (list): List of Answer Labels (e.g., A, B, C, D).\n#             texts (list): List of question texts.\n#             labels (list): List of true Misconception IDs.\n#             tokenizer (transformers tokenizer): Tokenizer to encode the text.\n#             max_length (int, optional): Max sequence length for tokenization. Defaults to 128.\n#         \"\"\"\n#         self.question_ids = question_ids\n#         self.answer_labels = answer_labels\n#         self.texts = texts\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n\n#     def __len__(self):\n#         return len(self.texts)\n\n#     def __getitem__(self, idx):\n#         question_id = self.question_ids[idx]\n#         answer_label = self.answer_labels[idx]\n#         text = self.texts[idx]\n\n#         # Assuming the `text` is just the question text; answer is inferred from `answer_label`\n#         question = text  # In case answer text is separate, modify as needed\n\n#         # Tokenize the question text\n#         tokens = self.tokenizer(\n#             question,\n#             padding=\"max_length\",\n#             truncation=True,\n#             max_length=self.max_length,\n#             return_tensors=\"pt\",\n#             return_special_tokens_mask=True,  # Ensures correct use of [SEP]\n#         )\n\n#         return {\n#             \"input_ids\": tokens[\"input_ids\"].squeeze(),\n#             \"attention_mask\": tokens[\"attention_mask\"].squeeze(),\n#             \"QuestionId\": question_id,  # Include the Question ID\n#             \"AnswerLabel\": answer_label  # Include the Answer label (A, B, C, D)\n#         }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.159545Z","iopub.status.idle":"2024-12-09T11:51:14.160248Z","shell.execute_reply.started":"2024-12-09T11:51:14.159992Z","shell.execute_reply":"2024-12-09T11:51:14.160018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Preprocess data\n# def preprocess_testdata(df):\n#     data = []\n#     for _, row in df.iterrows():\n#         for option in [\"A\", \"B\", \"C\", \"D\"]:  # Only incorrect answers\n#             # if row['CorrectAnswer'] != option:\n#             input_text = f\"Question: {row['QuestionText']} | Answer: {row[f'Answer{option}Text']}\"\n#             # label = row[f\"Misconception{option}Id\"]\n#             questionid = f\"{row['QuestionId']}\"\n#             answer = f\"{option}\"\n#             data.append((questionid, answer, input_text))\n#     return pd.DataFrame(data, columns=[\"QuestionId\",\"Answer\", \"text\"])\n\n# real_test_data = preprocess_testdata(real_test_df)\n# real_test_data.dropna(inplace = True)\n\n# # # Combine into DataFrames\n# # train_data = pd.DataFrame(train_data, columns=[\"text\", \"label\"])\n# # val_data = pd.DataFrame(val_data, columns=[\"text\", \"label\"])\n# # test_data = pd.DataFrame(test_data, columns=[\"text\", \"label\"])\n\n# # Convert labels to multi-hot encoding\n# # all_labels = sorted(misconception_df[\"MisconceptionId\"].unique())  # Get all unique labels\n# # mlb = MultiLabelBinarizer(classes=all_labels)\n\n# # real_test_labels = mlb.transform([[label] for label in real_test_data[\"label\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.161579Z","iopub.status.idle":"2024-12-09T11:51:14.162015Z","shell.execute_reply.started":"2024-12-09T11:51:14.161786Z","shell.execute_reply":"2024-12-09T11:51:14.161808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Create datasets\n# real_test_dataset = PredictionDataset(real_test_data[\"QuestionId\"].tolist(), real_test_data[\"Answer\"].tolist(), real_test_data[\"text\"].tolist(), tokenizer)\n\n# # DataLoader for batching\n# real_test_dataloader = DataLoader(real_test_dataset, batch_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.163764Z","iopub.status.idle":"2024-12-09T11:51:14.164231Z","shell.execute_reply.started":"2024-12-09T11:51:14.163990Z","shell.execute_reply":"2024-12-09T11:51:14.164012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Assuming you have already loaded the model and test_dataloader\n# model.load_state_dict(torch.load(best_model_path))\n# model.eval()\n\n# test_loss = 0\n# predictions, true_labels = [], []\n\n# # Store MAP@25 results\n# all_predictions = {}\n# all_true_labels = {}\n\n# with torch.no_grad():\n#     for batch in real_test_dataloader:\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         # labels = batch[\"labels\"].to(device)\n        \n#         # Forward pass\n#         outputs = model(input_ids, attention_mask=attention_mask)\n#         # test_loss += outputs.loss.item()\n\n#         logits = outputs.logits\n#         probabilities = torch.softmax(logits, dim=-1)\n\n#         # Get top 25 predictions (this can be adjusted for the actual number of misconceptions)\n#         top_k_predictions = torch.topk(probabilities, k=25, dim=1).indices.cpu().numpy()\n        \n#         # Store predictions and true labels\n#         for i, question_id in enumerate(batch[\"QuestionId\"]):  # Ensure batch contains 'QuestionId'\n#             question_answer = f\"{question_id}_{batch['AnswerLabel'][i]}\"  # Format QuestionId_Answer for unique identifier\n            \n#             all_predictions[question_answer] = top_k_predictions[i]  # Store top 25 predicted misconception ids\n#             all_true_labels[question_answer] = labels[i].cpu().numpy()  # Store true misconception ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.165444Z","iopub.status.idle":"2024-12-09T11:51:14.165716Z","shell.execute_reply.started":"2024-12-09T11:51:14.165585Z","shell.execute_reply":"2024-12-09T11:51:14.165600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Convert each NumPy array to a string\n# data_str = [(key, np.array2string(value)) for key, value in all_predictions.items()]\n# df = pd.DataFrame(data_str, columns=[\"QuestionId_Answer\", \"MisconceptionId\"])\n# df.to_csv(\"submission.csv\", columns=[\"QuestionId_Answer\", \"MisconceptionId\"], index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.166737Z","iopub.status.idle":"2024-12-09T11:51:14.167026Z","shell.execute_reply.started":"2024-12-09T11:51:14.166888Z","shell.execute_reply":"2024-12-09T11:51:14.166903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pd.read_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T11:51:14.168480Z","iopub.status.idle":"2024-12-09T11:51:14.168911Z","shell.execute_reply.started":"2024-12-09T11:51:14.168676Z","shell.execute_reply":"2024-12-09T11:51:14.168698Z"}},"outputs":[],"execution_count":null}]}